name: attention
hyperparams:
  aggregation:
    name: mean
  batch_size: 16
  epochs: 100
  optimizer_args:
    lr: 1e-4
    weight_decay: 1
  attn_hidden_dim: 27
  probe_architecture: simple_attention_pooling
  scheduler_decay: 0.2
