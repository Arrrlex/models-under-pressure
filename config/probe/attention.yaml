name: attention
hyperparams:
  batch_size: 128
  epochs: 20
  optimizer_args:
    lr: 5e-3
    weight_decay: 1e-3
  final_lr: 5e-4
  gradient_accumulation_steps: 1
  patience: 50
