name: attention
hyperparams:
  batch_size: 16
  epochs: 50
  device: "cpu"
  optimizer_args:
    lr: 1e-3
    weight_decay: 0.0004
  attn_hidden_dim: 27
  probe_architecture: "attention_then_linear"
  scheduler_decay: 0.62
