defaults:
  - probe: attention
  - eval_datasets: dev_balanced
  - model: llama-70b
  - baseline: llama3-1b-finetune
  - _self_

batch_size: 4
max_samples: null
random_seed: 42
train_data: prompts_4x
train_filters: null
compute_activations: false
validation_dataset: true

baselines:
  prompts:
    - default
    - single_letter
    - prompt_at_end
    - single_word

cv:
  layers: [5, 10, 15]
  folds: 4

generalisation_heatmap:
  variation_types:
    - prompt_style
    - tone
    - language
    - length

data_efficiency:
  dataset_sizes:
    - 2
    - 4
    - 8
    - 16
    - 32
    - 64
    - 128
    - 256
    - 512
    - 709

variation_types:
  - prompt_style
  - tone
  - language
  - length
