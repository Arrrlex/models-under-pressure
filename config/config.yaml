defaults:
  - probe: sklearn
  - eval_datasets: dev_balanced
  - model: llama-70b
  - _self_

layer: 31
batch_size: 4
max_samples: null
random_seed: 42
train_data: prompts_25_03_25_gpt-4o.jsonl
compute_activations: false
validation_dataset: false

baselines:
  prompts:
    - default
    - single_letter
    - prompt_at_end
    - single_word

cv:
  layers: [5, 10, 15]
  folds: 4

generalisation_heatmap:
  variation_types:
    - prompt_style
    - tone
    - language
    - length
