
model_name: "llama-1b"
training_data: "prompts_13_03_25_gpt-4o_filtered.jsonl"
batch_size: 4
cv_folds: 5
best_layer: 5
layers: [5, 6]
max_samples: 20
use_test_set: false
experiments_to_run:
  - compare_probes
default_hyper_params:
  C: 1e-3
  random_state: 42
  fit_intercept: false

probes:
  #- name: "sklearn_mean_agg_probe"
  #  hyper_params:
  #    C: 1e-2
  #    random_state: 42
  #    fit_intercept: false
  - name: "pytorch_per_token_probe"
    hyper_params:
      batch_size: 16
      epochs: 3
      device: "cpu"
  #- name: "lda"
  #  hyper_params:
  #    batch_size: 16
  #    epochs: 3
  #    device: "cpu"
  #- name: "difference_of_means"
  #  hyper_params:
  #    batch_size: 16
  #    epochs: 3
  #    device: "cpu"

best_probe:
  name: "sklearn_probe"

baseline_models:
  - llama-1b
  - gemma-1b

variation_types:
  - prompt_style
  - tone
  - language
  - length
