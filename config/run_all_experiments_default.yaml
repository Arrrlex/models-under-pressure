# @package _global_
model_name: "llama-1b"
train_data: "prompts_25_03_25_gpt-4o.jsonl"
batch_size: 4
cv_folds: 5
best_layer: 5
layers: [5, 6]
max_samples: 50
use_test_set: false
experiments_to_run:
  - cv
  - compare_probes
  - compare_best_probe_against_baseline
  - generalisation_heatmap
  - scaling_plot

default_hyperparams:
  C: 1e-3
  random_state: 42
  fit_intercept: false

probes:
  - name: sklearn_mean_agg_probe
  - name: pytorch_per_token_probe
    hyperparams:
      batch_size: 16
      epochs: 3
      device: "cpu"
      learning_rate: 1e-2
      weight_decay: 0.1
  - name: lda
    hyperparams:
      batch_size: 16
      epochs: 3
      device: "cpu"
  - name: difference_of_means
    hyperparams:
      batch_size: 16
      epochs: 3
      device: "cpu"

best_probe:
  name: sklearn_mean_agg_probe

baseline_models:
  - llama-1b
  - gemma-1b

baseline_prompts:
  - default

variation_types:
  - prompt_style
  - tone
  - language
  - length

scaling_plot:
  scaling_models:
    - llama-1b
    - gemma-1b

  scaling_layers:
    - 6
    - 16

  probe_spec:
    name: "sklearn_mean_agg_probe"
