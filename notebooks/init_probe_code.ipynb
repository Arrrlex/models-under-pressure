{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Probe Exploration\n",
    "\n",
    "### Goals:\n",
    "- Train a simple linear logistic regression probe on Llama-3-7b\n",
    "- Understand GPU capacity - can we do inference with 70B?\n",
    "- Look at the probe activations / test set classifications\n",
    "\n",
    "### Timeline:\n",
    "- Today (18/02/25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from jaxtyping import Float\n",
    "\n",
    "model_name = 'meta-llama/Llama-3.2-1B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset was generated using GPT-4o. It consists of 20 examples with red things and 20 examples with green things. We hope to learn a classifier / probe for green or red objects.\n",
    "\n",
    "`Command: Generate 20 sentences about red things. Generate 20 sentences about green things. Put them in a JSON array of strings.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading:\n",
    "text = [\n",
    "  \"The bright red apple hung low on the tree, ready to be picked.\",\n",
    "  \"A red sports car sped past, leaving a trail of dust behind.\",\n",
    "  \"The firefighter's uniform had reflective red stripes for visibility.\",\n",
    "  \"She wore a deep red dress that caught everyone's attention.\",\n",
    "  \"The red rose symbolized love and passion.\",\n",
    "  \"Tomatoes ripened under the sun, turning a rich shade of red.\",\n",
    "  \"The cardinal perched on the fence, its red feathers vibrant against the snow.\",\n",
    "  \"His face turned red with embarrassment after tripping on stage.\",\n",
    "  \"The sunset painted the sky in hues of red and orange.\",\n",
    "  \"Red chili peppers added a spicy kick to the dish.\",\n",
    "  \"The warning sign was painted bright red for safety reasons.\",\n",
    "  \"Her lipstick was a bold shade of red.\",\n",
    "  \"The red balloon floated away into the sky.\",\n",
    "  \"Blood is naturally red due to the presence of iron in hemoglobin.\",\n",
    "  \"Strawberries are at their sweetest when they turn fully red.\",\n",
    "  \"The red fire hydrant stood at the corner of the street.\",\n",
    "  \"Maple leaves turn a brilliant red in the autumn.\",\n",
    "  \"A red velvet cake is a delicious dessert with a hint of cocoa.\",\n",
    "  \"The ladybug crawled across the leaf, its red shell dotted with black spots.\",\n",
    "  \"Santa Claus is always dressed in his iconic red suit.\",\n",
    "  \"The fresh green grass covered the rolling hills.\",\n",
    "  \"A green traffic light signaled the cars to move forward.\",\n",
    "  \"Emeralds are precious gems with a deep green color.\",\n",
    "  \"The frog leaped into the pond, blending in with the green lily pads.\",\n",
    "  \"Spinach is a nutritious green vegetable rich in iron.\",\n",
    "  \"The soccer field was painted bright green for the championship game.\",\n",
    "  \"A bright green parrot perched on the branch, mimicking voices.\",\n",
    "  \"The cucumber felt cool and crisp in her hands.\",\n",
    "  \"The lush green rainforest was teeming with wildlife.\",\n",
    "  \"She wore a green jade bracelet that shimmered under the light.\",\n",
    "  \"Green tea is known for its numerous health benefits.\",\n",
    "  \"The traffic sign was painted green to indicate an exit route.\",\n",
    "  \"The chameleon changed its color to blend with the green leaves.\",\n",
    "  \"The avocado's skin turned dark green when fully ripe.\",\n",
    "  \"His green eyes sparkled in the sunlight.\",\n",
    "  \"The turtle slowly crawled across the green moss-covered rock.\",\n",
    "  \"The neon green sign stood out in the dimly lit alley.\",\n",
    "  \"Green grapes are sweet and slightly tangy when ripe.\",\n",
    "  \"The Christmas tree stood tall, covered in green pine needles.\",\n",
    "  \"The four-leaf clover is a rare green plant that symbolizes luck.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Feature Inputs for the Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 1, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 2, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 3, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 4, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 5, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 6, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 7, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 8, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 9, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 10, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 11, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 12, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 13, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 14, Activation Shape: torch.Size([40, 18, 2048])\n",
      "Layer: 15, Activation Shape: torch.Size([40, 18, 2048])\n",
      "All activations shape: torch.Size([16, 40, 18, 2048])\n"
     ]
    }
   ],
   "source": [
    "# Load the LLaMA-3-1B model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                   padding=True, max_length=1028)\n",
    "\n",
    "# Dictionary to store residual activations\n",
    "activations = []\n",
    "\n",
    "# Hook function to capture residual activations before layernorm\n",
    "def hook_fn(module, input, output):\n",
    "    activations.append(input[0].detach().cpu())  # Store the residual connection\n",
    "\n",
    "# Register hooks on each transformer block (LLaMA layers)\n",
    "hooks = []\n",
    "for i, layer in enumerate(model.model.layers):  # LLaMA uses model.model.layers\n",
    "    hook = layer.input_layernorm.register_forward_hook(hook_fn)  # Pre-attention residual\n",
    "    hooks.append(hook)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    _ = model(**inputs)\n",
    "\n",
    "# Remove hooks after capturing activations\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# Print stored activations\n",
    "for i, act in enumerate(activations):\n",
    "    print(f\"Layer: {i}, Activation Shape: {act.shape}\")\n",
    "\n",
    "all_acts = torch.stack(activations)\n",
    "print('All activations shape:', all_acts.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code for the Probe\n",
    "\n",
    "Use `sklearn` logistic regression classifier to learn a linear classifier on the activations from the model. We do the following:\n",
    "\n",
    "1. Create the y labels (1 for red and 0 for green)\n",
    "2. Restructure X to match sklearn (Batch_size, Embedd_dim) -> One per layer, final seq pos **TODO: Iterate in Future**  \n",
    "3. Run Logistic Regression\n",
    "4. Test on 5 test data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape=(40,)\n"
     ]
    }
   ],
   "source": [
    "labels = np.concatenate([np.ones(20), np.zeros(20)])\n",
    "print(f'{labels.shape=}')\n",
    "\n",
    "# Select the last sequence position activations:\n",
    "train_acts = activations[..., -1, :]\n",
    "\n",
    "# Define parallel fit function:\n",
    "def train_logistic_regression(activations: Float[np.ndarray, \"batch_size embedd_dim\"], labels: Float[np.ndarray, \"batch_size\"]) -> LogisticRegression:\n",
    "    \"\"\"\n",
    "    Train a logistic regression model on the residual activations of the LLaMA model. Class designed for parallel training with joblib\n",
    "    \"\"\"   \n",
    "    pass \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parallel = Parallel(n_jobs=4, backend='loky')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
