{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out how to Implement Per Token Probes \n",
    "\n",
    "\n",
    "#### Goal \n",
    "- Produce per token probe plots that visualise how the 'high-stakes' nature of the probe varies over words.\n",
    "- Figure out what this script needs to look like and apply it to an eval dataset from Figure 2/ a split from the variation types\n",
    "\n",
    "#### Timeline\n",
    "\n",
    "Created: 06/03/25 \n",
    "Ideally Finished: 07/03/25 to send to supervisors\n",
    "\n",
    "\n",
    "# Current Issue\n",
    "\n",
    "Problem in the probe result batching causing everything to be 228"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import json\n",
    "from models_under_pressure.config import SYNTHETIC_DATASET_PATH, TRAIN_TEST_SPLIT\n",
    "from models_under_pressure.experiments.dataset_splitting import load_train_test\n",
    "from models_under_pressure.probes.model import LLMModel\n",
    "from models_under_pressure.probes.probes import LinearProbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and dataset\n",
    "model = LLMModel.load(model_name=\"meta-llama/Llama-3.2-1B-Instruct\", device=\"cuda:2\")\n",
    "\n",
    "# Train a probe\n",
    "train_dataset, _ = load_train_test(\n",
    "    dataset_path=SYNTHETIC_DATASET_PATH,\n",
    "    split_path=TRAIN_TEST_SPLIT,\n",
    ")\n",
    "\n",
    "model.device = \"cuda:2\"\n",
    "print(\"Model Device:\", model.device)\n",
    "print(\"Model Internal Device:\", model.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the dataset used to train the probe:\n",
    "print(f\"Train dataset shape: {train_dataset.to_pandas().shape}\")\n",
    "print(\n",
    "    f\"Train dataset variation types: {train_dataset.to_pandas()['variation_type'].unique()}\"\n",
    ")\n",
    "filtered_dataset = train_dataset.filter(\n",
    "    lambda x: x.other_fields[\"variation_type\"] == \"prompt_style\"\n",
    ")\n",
    "print(f\"Filtered dataset shape: {filtered_dataset.to_pandas().shape}\")\n",
    "print(\n",
    "    f\"Filtered dataset variation types: {filtered_dataset.to_pandas()['variation_type'].unique()}\"\n",
    ")\n",
    "\n",
    "filtered_dataset.to_pandas().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the probe:\n",
    "probe = LinearProbe(_llm=model, layer=11)\n",
    "probe.fit(\n",
    "    train_dataset.filter(lambda x: x.other_fields[\"variation_type\"] == \"prompt_style\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the eval dataset of interest:\n",
    "from models_under_pressure.interfaces.dataset import Dataset\n",
    "from models_under_pressure.config import EVAL_DATASETS\n",
    "\n",
    "eval_dataset = Dataset.load_from(\n",
    "    EVAL_DATASETS[\"anthropic\"][\"file_path\"],\n",
    "    field_mapping=EVAL_DATASETS[\"anthropic\"][\"field_mapping\"],\n",
    ")\n",
    "\n",
    "# Print the dataset and look at it's shape:\n",
    "print(f\"Eval dataset shape: {eval_dataset.to_pandas().shape}\")\n",
    "eval_dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = probe.per_token_predictions(eval_dataset.inputs)\n",
    "# Add predictions as a column to the eval dataset dataframe\n",
    "df_eval = eval_dataset.to_pandas()\n",
    "df_eval[\"predictions\"] = list(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval[\"probe_probs\"] = (\n",
    "    df_eval[\"predictions\"].apply(lambda x: x[x != -1].tolist()).apply(json.dumps)\n",
    ")\n",
    "df_eval.head()\n",
    "\n",
    "df_eval.to_csv(\"../data/evals/anthropic_samples_per_token.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/evals/anthropic_samples_per_token.csv\")\n",
    "df[\"probe_probs\"] = df[\"probe_probs\"].apply(json.loads)\n",
    "print(df[\"probe_probs\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from typing import Dict\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "\n",
    "def tokenize_input(input_str: str) -> Dict[str, torch.Tensor]:\n",
    "    token_text = tokenizer.apply_chat_template(\n",
    "        json.loads(input_str), tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    token_dict = tokenizer(token_text, return_tensors=\"pt\")\n",
    "    return token_dict\n",
    "\n",
    "\n",
    "# Check the consistency of the probe probabilities and tokenized inputs:\n",
    "tokenized_inputs = df[\"inputs\"].apply(json.loads)\n",
    "print(tokenized_inputs[0])\n",
    "print(df[\"probe_probs\"][0])\n",
    "\n",
    "# Check the consistency of the probe probabilities and tokenized inputs:\n",
    "\n",
    "tokenized_inputs = df[\"inputs\"].apply(tokenize_input)\n",
    "print(f\"Length of tokenized inputs: {len(tokenized_inputs[0])}\")\n",
    "print(f\"Length of probe probs: {len(df['probe_probs'][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply chat template and then tokenize:\n",
    "print(tokenized_inputs[0][\"input_ids\"].shape)\n",
    "print(tokenizer.decode(tokenized_inputs[0][\"input_ids\"][0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply chat template and tokenize:\n",
    "tokenized_inputs2 = df[\"inputs\"].apply(\n",
    "    lambda x: tokenizer.apply_chat_template(\n",
    "        json.loads(x), tokenize=True, return_tensors=\"pt\", add_generation_prompt=True\n",
    "    )[0]\n",
    ")\n",
    "print(tokenized_inputs2[0].shape)\n",
    "print(tokenizer.decode(tokenized_inputs2[0].tolist(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval[\"predictions\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
