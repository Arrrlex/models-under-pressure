{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probe Generalisation MVP\n",
    "\n",
    "### Goals:\n",
    "- [x] Choose a layer to train linear probes at\n",
    "  - For now, we're doing layer 5, since that's the earliest layer that got perfect accuracy in the initial probe exploration\n",
    "- [x] For each category in the 19th Feb dataset, train a linear probe\n",
    "- [x] Generate a heatmap plot, where the (x, y)-th entry is the accuracy of the probe trained on x data, predicted on y data\n",
    "- [x] Understand GPU capacity - can we do inference with 70B?\n",
    "  - Yes, but it takes 2 A100 80GB GPUs when using bfloat16\n",
    "\n",
    "### Timeline:\n",
    "- 19/02/25 and 20/02/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython magic commands for autoreloading modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "from models_under_pressure.probes import (\n",
    "    create_activations,\n",
    "    train_single_layer,\n",
    "    compute_accuracy,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "project_root = Path(\"..\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading 20 Feb dataset\n",
    "import json\n",
    "\n",
    "feb_20_data = []\n",
    "with open(project_root / \"temp_data/dataset_21_feb.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        feb_20_data.append(json.loads(line))\n",
    "df = pd.DataFrame(feb_20_data)\n",
    "\n",
    "df[\"top_category\"] = df[\"category\"]\n",
    "df[\"prompt_text\"] = df[\"prompt\"]\n",
    "\n",
    "print(f\"dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading 19th Feb dataset\n",
    "df = pd.read_csv(project_root / \"temp_data/dataset_19_feb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data by top category\n",
    "categories = {}\n",
    "for category in df[\"top_category\"].unique():\n",
    "    category_df = df[df[\"top_category\"] == category]\n",
    "    categories[category] = {\n",
    "        \"X\": category_df[\"prompt_text\"].tolist(),\n",
    "        \"y\": category_df[\"high_stakes\"].tolist(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "cache_dir = \"/scratch/ucabwjn/.cache\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# Load the LLaMA-3-1B model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    max_memory={0: \"70GB\", 1: \"70GB\", 3: \"70GB\"},\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on each category's data, recording the activations\n",
    "\n",
    "for category in categories:\n",
    "    categories[category][\"acts\"] = create_activations(\n",
    "        model=model, tokenizer=tokenizer, text=categories[category][\"X\"], device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the categories[category]['accuracy'] values in a heatmap\n",
    "\n",
    "\n",
    "model_params = {\"C\": 1, \"random_state\": 42, \"fit_intercept\": False}\n",
    "\n",
    "# For each category, train a linear probe on the activations for layer 5:\n",
    "# for category in categories:\n",
    "#     categories[category]['probes'] = Parallel(n_jobs=16)(\n",
    "#     delayed(train_single_layer)(acts, categories[category]['y'], model_params) for acts in categories[category][\"acts\"]\n",
    "# )\n",
    "\n",
    "for category in tqdm(categories, desc=\"Training probes for categories\"):\n",
    "    categories[category][\"probes\"] = Parallel(n_jobs=16)(\n",
    "        delayed(train_single_layer)(acts, categories[category][\"y\"], model_params)\n",
    "        for acts in tqdm(\n",
    "            categories[category][\"acts\"],\n",
    "            desc=f\"Layer probes for {category}\",\n",
    "            leave=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "accuracies = []\n",
    "for cat1 in categories:\n",
    "    for cat2 in categories:\n",
    "        accuracy = [\n",
    "            compute_accuracy(\n",
    "                probe=probe,  # categories[cat1][\"probe\"],\n",
    "                activations=acts,  # categories[cat2][\"acts\"][5],\n",
    "                labels=categories[cat2][\"y\"],\n",
    "            )\n",
    "            for probe, acts in zip(categories[cat1][\"probes\"], categories[cat2][\"acts\"])\n",
    "        ]\n",
    "        accuracies.append(\n",
    "            {\n",
    "                \"probe\": cat1,\n",
    "                \"dataset\": cat2,\n",
    "                \"accuracy\": accuracy,\n",
    "            }\n",
    "        )\n",
    "accuracies = pd.DataFrame(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- We need to fix y axis labels and make it clear which axis is probes and which is dataset\n",
    "- We need a better colour scheme: red should be bad!\n",
    "- Clearly communicate the experimental procedure\n",
    "- Put all of this in a doc with some time before the meeting tomorrow (20th Feb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [5, 22, 50, 79]\n",
    "\n",
    "\n",
    "for layer in layers:\n",
    "    accuracies[f\"layer_{layer}_accuracy\"] = accuracies[\"accuracy\"].apply(\n",
    "        lambda x: x[layer]\n",
    "    )\n",
    "    # Pivot the accuracies DataFrame to create a matrix\n",
    "    accuracy_matrix = accuracies.pivot(\n",
    "        index=\"dataset\", columns=\"probe\", values=f\"layer_{layer}_accuracy\"\n",
    "    )\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        accuracy_matrix,\n",
    "        annot=True,  # Show values in cells\n",
    "        fmt=\".3f\",  # Format numbers to 3 decimal places\n",
    "        cmap=\"RdBu\",  # Red (0) to Blue (1)\n",
    "        vmin=0,  # Force scale to start at 0\n",
    "        vmax=1,  # Force scale to end at 1\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Probe Generalization Across Categories, layer: {layer}\")\n",
    "    plt.xlabel(\"Probe Category\")\n",
    "    plt.ylabel(\"Test Dataset\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(project_root / \"plots/probe_generalisation_heatmap.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Completions On-Policy\n",
    "\n",
    "To ensure the quality of the dataset we analyse completions to the prompts by the model we \n",
    "are probing. Our goal is to ensure the dataset actually captures high stakes situations,\n",
    "instead of some confounding factor e.g. likely to generate the word \"wait!\" becuase of the type\n",
    "of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_under_pressure.utils import generate_completions\n",
    "\n",
    "df[\"completions\"] = generate_completions(model, tokenizer, df[\"prompt\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of prompt-completion pairs\n",
    "pd.set_option(\"display.max_colwidth\", None)  # Show full text\n",
    "sample_size = 5\n",
    "sample_df = df[[\"prompt\", \"completions\"]].sample(n=sample_size, random_state=42)\n",
    "\n",
    "print(\"Sample of Prompt-Completion Pairs:\\n\")\n",
    "for idx, row in sample_df.iterrows():\n",
    "    print(f\"Prompt {idx}:\\n{row['prompt']}\\n\")\n",
    "    print(f\"Completion:\\n{row['completions']}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset with completions\n",
    "output_path = project_root / \"temp_data/dataset_21-02-2025_completions.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Saved dataset with completions to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
