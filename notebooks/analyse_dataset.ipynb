{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Dataset\n",
    "\n",
    "Our goal in this project is to analyse how well model probes can generalise to new tasks. A downside of this is that we need to ensure the datasets accurately capture high stakes situations. We analyse a series of statistics in this notebook to ensure the dataset is of high quality.\n",
    "\n",
    "## Goal:\n",
    "\n",
    "- [x] Generate completions for each prompt in the dataset.\n",
    "- [x] Analyse the completions to ensure there aren't confounding factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from models_under_pressure.utils import generate_completions\n",
    "\n",
    "project_root = Path(\"..\").resolve()\n",
    "print(f'Current Working Directory:{os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from jsonl file\n",
    "dataset_path = project_root / \"temp_data/dataset_21_feb.jsonl\"\n",
    "df = pd.read_json(dataset_path, lines=True)\n",
    "print(f\"Loaded dataset with {len(df)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with completions\n",
    "completions_path = project_root / \"temp_data/dataset_21-02-2025_completions.csv\"\n",
    "df = pd.read_csv(completions_path)\n",
    "print(f\"Loaded dataset with {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "cache_dir = '/scratch/ucabwjn/.cache'\n",
    "device = 'cuda:0'\n",
    "\n",
    "# Load the LLaMA-3-1B model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"auto\",\n",
    "                                             max_memory={0: \"70GB\", 1: \"70GB\", 3: \"70GB\"},\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             cache_dir=cache_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate completions\n",
    "df['completions'] = generate_completions(model, tokenizer, df['prompt'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of prompt-completion pairs\n",
    "pd.set_option('display.max_colwidth', None)  # Show full text\n",
    "sample_size = 5\n",
    "sample_df = df[['prompt', 'completions']].sample(n=sample_size, random_state=42)\n",
    "\n",
    "print(\"Sample of Prompt-Completion Pairs:\\n\")\n",
    "for idx, row in sample_df.iterrows():\n",
    "    print(f\"Prompt {idx}:\\n{row['prompt']}\\n\")\n",
    "    print(f\"Completion:\\n{row['completions']}\\n\")\n",
    "    print(\"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset with completions\n",
    "output_path = project_root / \"temp_data/dataset_21-02-2025_completions.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Saved dataset with completions to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a word cloud for the completions and prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with completions\n",
    "df = pd.read_csv(project_root / \"temp_data/dataset_21-02-2025_completions.csv\")\n",
    "print(f\"Loaded dataset with {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get unique categories\n",
    "categories = df['category'].unique()\n",
    "\n",
    "# Create word clouds for prompts\n",
    "for category in categories:\n",
    "    # Filter prompts for this category\n",
    "    category_text = ' '.join(df[df['category'] == category]['prompt'].tolist())\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(category_text)\n",
    "    \n",
    "    # Create new figure for this category\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud of Prompts - {category}')\n",
    "    plt.show()\n",
    "\n",
    "# Create word clouds for completions\n",
    "for category in categories:\n",
    "    # Filter completions for this category\n",
    "    category_text = ' '.join(df[df['category'] == category]['completions'].tolist())\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(category_text)\n",
    "    \n",
    "    # Create new figure for this category\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud of Completions - {category}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in high_stakes column\n",
    "print(df['high_stakes'].unique())\n",
    "\n",
    "# Convert high_stakes column to numeric values\n",
    "df['high_stakes'] = df['high_stakes'].map({'0': 0, '1': 1, 'False': 0, 'True': 1})\n",
    "\n",
    "print(df['high_stakes'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a specific category (e.g., first category)\n",
    "category = df['category'].unique()[0]\n",
    "\n",
    "# Create word clouds for high stakes vs low stakes prompts\n",
    "for stakes in [0, 1]:\n",
    "    # Filter prompts for this category and stakes level\n",
    "    stakes_text = ' '.join(df[(df['category'] == category) & \n",
    "                             (df['high_stakes'] == stakes)]['prompt'].tolist())\n",
    "        \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(stakes_text)\n",
    "    \n",
    "    # Create new figure\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud of Prompts - {category} (High Stakes: {stakes})')\n",
    "    plt.show()\n",
    "\n",
    "# Create word clouds for high stakes vs low stakes completions \n",
    "for stakes in [0, 1]:\n",
    "    # Filter completions for this category and stakes level\n",
    "    stakes_text = ' '.join(df[(df['category'] == category) & \n",
    "                             (df['high_stakes'] == stakes)]['completions'].tolist())\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(stakes_text)\n",
    "    \n",
    "    # Create new figure\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud of Completions - {category} (High Stakes: {stakes})')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Get a specific category (e.g., first category)\n",
    "category = df['category'].unique()[0]\n",
    "\n",
    "# Create TF-IDF word clouds for high stakes vs low stakes prompts\n",
    "for stakes in [0, 1]:\n",
    "    # Filter prompts for this category and stakes level\n",
    "    prompts = df[(df['category'] == category) & \n",
    "                 (df['high_stakes'] == stakes)]['prompt'].tolist()\n",
    "    \n",
    "    # Calculate TF-IDF\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf.fit_transform(prompts)\n",
    "    \n",
    "    # Get words and their TF-IDF scores\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "    \n",
    "    # Create dictionary of word frequencies based on TF-IDF scores\n",
    "    word_freq = dict(zip(feature_names, tfidf_scores))\n",
    "    \n",
    "    # Generate word cloud using TF-IDF frequencies\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "    \n",
    "    # Create new figure\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'TF-IDF Word Cloud of Prompts - {category} (High Stakes: {stakes})')\n",
    "    plt.show()\n",
    "\n",
    "# Create TF-IDF word clouds for high stakes vs low stakes completions\n",
    "for stakes in [0, 1]:\n",
    "    # Filter completions for this category and stakes level\n",
    "    completions = df[(df['category'] == category) & \n",
    "                    (df['high_stakes'] == stakes)]['completions'].tolist()\n",
    "    \n",
    "    # Calculate TF-IDF\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf.fit_transform(completions)\n",
    "    \n",
    "    # Get words and their TF-IDF scores\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "    \n",
    "    # Create dictionary of word frequencies based on TF-IDF scores\n",
    "    word_freq = dict(zip(feature_names, tfidf_scores))\n",
    "    \n",
    "    # Generate word cloud using TF-IDF frequencies\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "    \n",
    "    # Create new figure\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'TF-IDF Word Cloud of Completions - {category} (High Stakes: {stakes})')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
